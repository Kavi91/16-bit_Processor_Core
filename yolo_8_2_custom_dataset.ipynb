{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kavi91/16-bit_Processor_Core/blob/main/yolo_8_2_custom_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Nc1siTRzMV8"
      },
      "outputs": [],
      "source": [
        "#pip install ultralytics"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard"
      ],
      "metadata": {
        "id": "VZ_16HCV6ItQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "import requests\n",
        "from tqdm.auto import tqdm"
      ],
      "metadata": {
        "id": "mFiLXIud0VEv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download dataset function\n",
        "def download_file(url, save_name):\n",
        "    if not os.path.exists(save_name):\n",
        "        print(f\"Downloading file\")\n",
        "        file = requests.get(url, stream=True)\n",
        "        total_size = int(file.headers.get('content-length', 0))\n",
        "        block_size = 1024\n",
        "        progress_bar = tqdm(\n",
        "            total=total_size,\n",
        "            unit='iB',\n",
        "            unit_scale=True\n",
        "        )\n",
        "        with open(os.path.join(save_name), 'wb') as f:\n",
        "            for data in file.iter_content(block_size):\n",
        "                progress_bar.update(len(data))\n",
        "                f.write(data)\n",
        "        progress_bar.close()\n",
        "    else:\n",
        "        print('File already present')\n",
        "\n",
        "download_file(\n",
        "    'https://www.dropbox.com/s/xc2890eh8ujy3cu/hituav-a-highaltitude-infrared-thermal-dataset.zip?dl=1',\n",
        "    'hituav-a-highaltitude-infrared-thermal-dataset.zip'\n",
        ")"
      ],
      "metadata": {
        "id": "7_zvmmnj0Z01"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Support for third party widgets will remain active for the duration of the session. To disable support:"
      ],
      "metadata": {
        "id": "YhMTiOUG9Yd5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Support for third party widgets will remain active for the duration of the session. To disable support:"
      ],
      "metadata": {
        "id": "B3qQ8HJJ9X7n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ekyyfuLq9cZH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "\n",
        "def create_dir(dir_name=\"dataset\"):\n",
        "  \"\"\"Creates a directory if it does not exist.\n",
        "\n",
        "  Args:\n",
        "    dir_name: The name of the directory to create.\n",
        "  \"\"\"\n",
        "  if not os.path.exists(dir_name):\n",
        "    os.makedirs(dir_name)\n",
        "    print(f\"Created directory: {dir_name}\")\n",
        "  else:\n",
        "    print(f\"Directory {dir_name} already exists.\")\n",
        "\n",
        "def unzip(zip_file=None):\n",
        "  \"\"\"Unzips a file to the specified directory.\n",
        "\n",
        "  Args:\n",
        "    zip_file: The path to the zip file to extract.\n",
        "  \"\"\"\n",
        "  if not zip_file or not os.path.isfile(zip_file):\n",
        "    print(\"File not found or invalid file path provided.\")\n",
        "    return\n",
        "\n",
        "  try:\n",
        "    create_dir()  # Create the directory before extraction\n",
        "    with zipfile.ZipFile(zip_file, 'r') as z:\n",
        "      z.extractall(\"./dataset\")\n",
        "      print(\"Extracted all files to the dataset directory.\")\n",
        "  except zipfile.BadZipFile:\n",
        "    print(\"Invalid zip file or file is corrupted.\")\n",
        "  except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n",
        "\n",
        "# Unzip the provided dataset (replace with your actual file path)\n",
        "unzip('hituav-a-highaltitude-infrared-thermal-dataset.zip')\n"
      ],
      "metadata": {
        "id": "oEWG8deT00s3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from ultralytics import YOLO\n",
        "\n",
        "# Override settings by directly setting them in the environment\n",
        "os.environ[\"YOLO_DATASETS_DIR\"] = \"hit-uav/images/\"\n"
      ],
      "metadata": {
        "id": "Z5AHPeHD8t4F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import yaml\n",
        "\n",
        "# Define the base path for WSL\n",
        "new_base_path = \"/home/kavinda/dataset/hit-uav\"\n",
        "\n",
        "# Load the YAML data\n",
        "with open(f\"{new_base_path}/dataset.yaml\", 'r') as file:\n",
        "    data = yaml.safe_load(file)\n",
        "\n",
        "# Update the paths\n",
        "data['path'] = new_base_path\n",
        "data['train'] = \"images/train\"\n",
        "data['val'] = \"images/val\"\n",
        "data['test'] = \"images/test\"\n",
        "\n",
        "# Save the updated data\n",
        "with open(f\"{new_base_path}/dataset.yaml\", 'w') as file:\n",
        "    yaml.dump(data, file, default_flow_style=False)\n",
        "\n",
        "print(\"YAML file updated successfully!\")\n",
        "print(yaml.dump(data, default_flow_style=False))\n"
      ],
      "metadata": {
        "id": "jmDkBvzm8tw_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ttR61ucr8tc7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "base_path = \"/home/kavinda/dataset/hit-uav\"\n",
        "\n",
        "# List the contents of the dataset directory\n",
        "print(os.listdir(base_path))\n",
        "\n",
        "# List the contents of the images directory\n",
        "print(os.listdir(os.path.join(base_path, \"images\")))\n"
      ],
      "metadata": {
        "id": "mCMmT0hk_xoo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the hyperparameters and their values\n",
        "hyperparameters = {\n",
        "    \"model\": None,  # Path to pretrained model or YAML config\n",
        "    \"data\": \"/home/kavinda/dataset/hit-uav/dataset.yaml\",  # Dataset configuration file\n",
        "    \"epochs\": 100,  # Number of epochs to train\n",
        "    \"time\": None,  # Time limit for training\n",
        "    \"patience\": 100,  # Early stopping patience\n",
        "    \"batch\": 32,  # Number of samples per gradient update\n",
        "    \"imgsz\": 640,  # Input image size\n",
        "    \"save\": True,  # Whether to save the model\n",
        "    \"save_period\": -1,  # Period for saving checkpoints (-1 means no periodic saving)\n",
        "    \"cache\": True,  # Cache images for faster training\n",
        "    \"device\": \"\",  # Device to run the training on (e.g., '0' for GPU 0)\n",
        "    \"workers\": 8,  # Number of dataloader workers\n",
        "    \"project\": \"runs/train\",  # Project name for saving results\n",
        "    \"name\": \"exp\",  # Name of the experiment\n",
        "    \"exist_ok\": False,  # Allow existing project/name directory\n",
        "    \"pretrained\": True,  # Use pretrained weights\n",
        "    \"optimizer\": \"AdamW\",  # Optimizer used for training\n",
        "    \"verbose\": False,  # Verbose output during training\n",
        "    \"seed\": 0,  # Random seed for reproducibility\n",
        "    \"deterministic\": True,  # Deterministic training for reproducibility\n",
        "    \"single_cls\": False,  # Train as single-class dataset\n",
        "    \"rect\": False,  # Rectangular training flag\n",
        "    \"cos_lr\": True,  # Use cosine learning rate scheduler\n",
        "    \"close_mosaic\": 10,  # Mosaic augmentation close interval\n",
        "    \"resume\": False,  # Resume training from the last checkpoint\n",
        "    \"amp\": True,  # Automatic Mixed Precision (AMP) training\n",
        "    \"fraction\": 1.0,  # Fraction of the dataset to use\n",
        "    \"profile\": False,  # Enable profiling\n",
        "    \"freeze\": None,  # Layers to freeze during training (None means no freezing)\n",
        "    \"lr0\": 0.0015,  # Initial learning rate\n",
        "    \"lrf\": 0.01,  # Final learning rate\n",
        "    \"momentum\": 0.937,  # Momentum factor for the optimizer\n",
        "    \"weight_decay\": 0.001,  # Weight decay (L2 penalty)\n",
        "    \"warmup_epochs\": 5.0,  # Number of warmup epochs\n",
        "    \"warmup_momentum\": 0.8,  # Warmup momentum\n",
        "    \"warmup_bias_lr\": 0.1,  # Warmup bias learning rate\n",
        "    \"box\": 10,  # Box loss gain\n",
        "    \"cls\": 1,  # Class loss gain\n",
        "    \"dfl\": 1.5,  # Distribution focal loss gain\n",
        "    \"pose\": 12.0,  # Pose loss gain\n",
        "    \"kobj\": 2.0,  # Keypoint object loss gain\n",
        "    \"label_smoothing\": 0.0,  # Label smoothing factor\n",
        "    \"nbs\": 64,  # Nominal batch size\n",
        "    \"overlap_mask\": True,  # Overlap mask flag\n",
        "    \"mask_ratio\": 4,  # Mask ratio\n",
        "    \"dropout\": 0.0,  # Dropout rate\n",
        "    \"val\": True,  # Enable validation\n",
        "    \"plots\": False,  # Enable plotting during training\n",
        "    \"hsv_h\": 0.015,  # HSV-Hue augmentation\n",
        "    \"hsv_s\": 0.7,  # HSV-Saturation augmentation\n",
        "    \"hsv_v\": 0.4,  # HSV-Value augmentation\n",
        "    \"degrees\": 0.0,  # Image rotation augmentation\n",
        "    \"translate\": 0.1,  # Image translation augmentation\n",
        "    \"scale\": 0.5,  # Image scale augmentation\n",
        "    \"shear\": 0.0,  # Image shear augmentation\n",
        "    \"perspective\": 0.0,  # Image perspective augmentation\n",
        "    \"flipud\": 0.0,  # Image flip up-down augmentation\n",
        "    \"fliplr\": 0.5,  # Image flip left-right augmentation\n",
        "    \"bgr\": 0.0,  # BGR augmentation\n",
        "    \"mosaic\": 1.0,  # Mosaic augmentation\n",
        "    \"mixup\": 0.1,  # Mixup augmentation\n",
        "    \"copy_paste\": 0.0,  # Copy-paste augmentation\n",
        "    \"auto_augment\": \"randaugment\",  # Auto augmentation method\n",
        "    \"erasing\": 0.4,  # Random erasing probability\n",
        "    \"crop_fraction\": 1.0  # Fraction of the image to crop\n",
        "}\n",
        "\n",
        "# Convert the dictionary to a DataFrame and write to an Excel file\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Define the file path\n",
        "file_path = 'parameter_history/hyperparameters.xlsx'\n",
        "\n",
        "# Check if the file exists and rename if necessary\n",
        "if os.path.exists(file_path):\n",
        "    base, extension = os.path.splitext(file_path)\n",
        "    counter = 1\n",
        "    new_file_path = f\"{base}_{counter}{extension}\"\n",
        "    while os.path.exists(new_file_path):\n",
        "        counter += 1\n",
        "        new_file_path = f\"{base}_{counter}{extension}\"\n",
        "    file_path = new_file_path\n",
        "\n",
        "df = pd.DataFrame(list(hyperparameters.items()), columns=['Hyperparameter', 'Value'])\n",
        "df.to_excel(file_path, index=False)\n",
        "\n",
        "print(f\"Excel file '{file_path}' has been created.\")\n"
      ],
      "metadata": {
        "id": "x3VGiGx5Wl-z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Map the hyperparameters from the previous cell\n",
        "manual_config = hyperparameters\n",
        "\n",
        "print(manual_config)\n"
      ],
      "metadata": {
        "id": "H9opgt-6VroV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from ultralytics import YOLO\n",
        "from tensorboard.plugins.hparams import api as hp\n",
        "import tensorboard\n",
        "\n",
        "# Load the YOLOv8 model\n",
        "model = YOLO('runs/train/exp3/weights/best.pt')\n",
        "\n",
        "# Print a statement indicating the configuration is not accessible\n",
        "print(\"Original Configuration is not accessible directly.\")\n",
        "\n",
        "# Set the log directory for TensorBoard\n",
        "log_dir = \"/home/kavinda/runs/logs\"\n",
        "os.makedirs(log_dir, exist_ok=True)\n",
        "\n",
        "# Train the model with the specified data and modified config\n",
        "model.train(\n",
        "    model=manual_config[\"model\"],\n",
        "    data=manual_config[\"data\"],\n",
        "    epochs=manual_config[\"epochs\"],\n",
        "    time=manual_config[\"time\"],\n",
        "    patience=manual_config[\"patience\"],\n",
        "    batch=manual_config[\"batch\"],\n",
        "    imgsz=manual_config[\"imgsz\"],\n",
        "    save=manual_config[\"save\"],\n",
        "    save_period=manual_config[\"save_period\"],\n",
        "    cache=manual_config[\"cache\"],\n",
        "    device=manual_config[\"device\"],\n",
        "    workers=manual_config[\"workers\"],\n",
        "    project=manual_config[\"project\"],\n",
        "    name=manual_config[\"name\"],\n",
        "    exist_ok=manual_config[\"exist_ok\"],\n",
        "    pretrained=manual_config[\"pretrained\"],\n",
        "    optimizer=manual_config[\"optimizer\"],\n",
        "    verbose=manual_config[\"verbose\"],\n",
        "    seed=manual_config[\"seed\"],\n",
        "    deterministic=manual_config[\"deterministic\"],\n",
        "    single_cls=manual_config[\"single_cls\"],\n",
        "    rect=manual_config[\"rect\"],\n",
        "    cos_lr=manual_config[\"cos_lr\"],\n",
        "    close_mosaic=manual_config[\"close_mosaic\"],\n",
        "    resume=manual_config[\"resume\"],\n",
        "    amp=manual_config[\"amp\"],\n",
        "    fraction=manual_config[\"fraction\"],\n",
        "    profile=manual_config[\"profile\"],\n",
        "    freeze=manual_config[\"freeze\"],\n",
        "    lr0=manual_config[\"lr0\"],\n",
        "    lrf=manual_config[\"lrf\"],\n",
        "    momentum=manual_config[\"momentum\"],\n",
        "    weight_decay=manual_config[\"weight_decay\"],\n",
        "    warmup_epochs=manual_config[\"warmup_epochs\"],\n",
        "    warmup_momentum=manual_config[\"warmup_momentum\"],\n",
        "    warmup_bias_lr=manual_config[\"warmup_bias_lr\"],\n",
        "    box=manual_config[\"box\"],\n",
        "    cls=manual_config[\"cls\"],\n",
        "    dfl=manual_config[\"dfl\"],\n",
        "    pose=manual_config[\"pose\"],\n",
        "    kobj=manual_config[\"kobj\"],\n",
        "    label_smoothing=manual_config[\"label_smoothing\"],\n",
        "    nbs=manual_config[\"nbs\"],\n",
        "    overlap_mask=manual_config[\"overlap_mask\"],\n",
        "    mask_ratio=manual_config[\"mask_ratio\"],\n",
        "    dropout=manual_config[\"dropout\"],\n",
        "    val=manual_config[\"val\"],\n",
        "    plots=manual_config[\"plots\"],\n",
        "    hsv_h=manual_config[\"hsv_h\"],\n",
        "    hsv_s=manual_config[\"hsv_s\"],\n",
        "    hsv_v=manual_config[\"hsv_v\"],\n",
        "    degrees=manual_config[\"degrees\"],\n",
        "    translate=manual_config[\"translate\"],\n",
        "    scale=manual_config[\"scale\"],\n",
        "    shear=manual_config[\"shear\"],\n",
        "    perspective=manual_config[\"perspective\"],\n",
        "    flipud=manual_config[\"flipud\"],\n",
        "    fliplr=manual_config[\"fliplr\"],\n",
        "    bgr=manual_config[\"bgr\"],\n",
        "    mosaic=manual_config[\"mosaic\"],\n",
        "    mixup=manual_config[\"mixup\"],\n",
        "    copy_paste=manual_config[\"copy_paste\"],\n",
        "    auto_augment=manual_config[\"auto_augment\"],\n",
        "    erasing=manual_config[\"erasing\"],\n",
        "    crop_fraction=manual_config[\"crop_fraction\"]\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Start TensorBoard\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir /home/kavinda/runs/logs\n"
      ],
      "metadata": {
        "id": "LNexruh60DfI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Validate the model\n",
        "metrics = model.val(plots=True)  # no arguments needed, dataset and settings remembered\n",
        "\n",
        "# Mean Average Precision (mAP) metrics\n",
        "map_50_95 = metrics.box.map       # mAP50-95\n",
        "map_50 = metrics.box.map50        # mAP50\n",
        "map_75 = metrics.box.map75        # mAP75\n",
        "maps_per_class = metrics.box.maps # List containing mAP50-95 for each category\n",
        "\n",
        "# Precision and Recall metrics\n",
        "precision = metrics.box.p         # Precision for each class\n",
        "recall = metrics.box.r            # Recall for each class\n",
        "\n",
        "# F1 Score\n",
        "f1_score = metrics.box.f1         # F1 score for each class\n",
        "\n",
        "# AP scores for all classes and all IoU thresholds\n",
        "all_ap = metrics.box.all_ap           # AP scores for all classes and all IoU thresholds\n",
        "ap_class_index = metrics.box.ap_class_index  # Index of class for each AP score\n",
        "\n",
        "# Print the metrics\n",
        "print(f\"mAP50-95: {map_50_95}\")\n",
        "print(f\"mAP50: {map_50}\")\n",
        "print(f\"mAP75: {map_75}\")\n",
        "print(f\"mAP per class: {maps_per_class}\")\n",
        "\n",
        "print(f\"Precision per class: {precision}\")\n",
        "print(f\"Recall per class: {recall}\")\n",
        "print(f\"F1 Score per class: {f1_score}\")\n",
        "\n",
        "print(f\"AP scores for all classes and all IoU thresholds: {all_ap}\")\n",
        "print(f\"Index of class for each AP score: {ap_class_index}\")\n"
      ],
      "metadata": {
        "id": "VPwAJe6hnLFE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Define the path to the video file\n",
        "input_video_path = 'DJI_20240517133159_0004_T.MP4'\n",
        "\n",
        "# Check if the file exists\n",
        "if os.path.exists(input_video_path):\n",
        "    print(\"File found.\")\n",
        "else:\n",
        "    print(\"File not found.\")\n"
      ],
      "metadata": {
        "id": "VSX6R4g4QA7-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import cv2\n",
        "from ultralytics import YOLO\n",
        "\n",
        "# Load the trained model\n",
        "best_model_path = \"runs/train/exp3/weights/best.pt\"\n",
        "best_model = YOLO(best_model_path)\n",
        "\n",
        "# Determine the device to use\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Set the model to the appropriate device\n",
        "best_model.to(device)\n",
        "\n",
        "# Define input and output video paths\n",
        "input_video_path = 'Input_videos/DJI_20240517132340_0002_T.MP4'\n",
        "output_video_base = \"results/result_4\"\n",
        "output_video_ext = \".mp4\"\n",
        "\n",
        "# Function to find a non-conflicting output path\n",
        "def get_non_conflicting_path(base, ext):\n",
        "    counter = 1\n",
        "    output_path = f\"{base}{ext}\"\n",
        "    while os.path.exists(output_path):\n",
        "        output_path = f\"{base}_{counter}{ext}\"\n",
        "        counter += 1\n",
        "    return output_path\n",
        "\n",
        "output_video_path = get_non_conflicting_path(output_video_base, output_video_ext)\n",
        "\n",
        "# Run the prediction\n",
        "results = best_model.predict(input_video_path, save=False, stream=True, conf = 0.5)  # Do not save each frame individually\n",
        "\n",
        "# Open the input video to read frames\n",
        "input_video = cv2.VideoCapture(input_video_path)\n",
        "frame_width = int(input_video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "frame_height = int(input_video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "fps = int(input_video.get(cv2.CAP_PROP_FPS))\n",
        "\n",
        "# Define the codec and create VideoWriter object\n",
        "fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # codec\n",
        "out = cv2.VideoWriter(output_video_path, fourcc, fps, (frame_width, frame_height))\n",
        "\n",
        "# Iterate over each frame in the results and write to output video\n",
        "for result in results:\n",
        "    frame = result.orig_img  # Original image\n",
        "    boxes = result.boxes  # Get the detection boxes\n",
        "\n",
        "    if boxes is not None:\n",
        "        for box in boxes:  # Iterate over each box\n",
        "            x1, y1, x2, y2 = map(int, box.xyxy[0])  # Get the coordinates\n",
        "            conf = box.conf[0]  # Confidence\n",
        "            cls = box.cls[0]  # Class\n",
        "\n",
        "            label = f'{best_model.names[int(cls)]} {conf:.2f}'\n",
        "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "            cv2.putText(frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
        "\n",
        "    out.write(frame)\n",
        "\n",
        "# Release everything when job is finished\n",
        "input_video.release()\n",
        "out.release()\n",
        "\n",
        "print(f\"Video processing completed. Saved to {output_video_path}.\")\n"
      ],
      "metadata": {
        "id": "W9ey1cWXOXOc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "from ultralytics import YOLO\n",
        "\n",
        "# Load the trained model\n",
        "best_model_path = \"runs/train/exp3/weights/best.pt\"\n",
        "best_model = YOLO(best_model_path)\n",
        "\n",
        "# Determine the device to use\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Set the model to the appropriate device\n",
        "best_model.to(device)\n",
        "\n",
        "# Define input and output video paths\n",
        "input_video_path = 'DJI_20240517132340_0002_T.MP4'\n",
        "output_video_base = \"result_3\"\n",
        "output_video_ext = \".mp4\"\n",
        "\n",
        "# Function to find a non-conflicting output path\n",
        "def get_non_conflicting_path(base, ext):\n",
        "    counter = 1\n",
        "    output_path = f\"{base}{ext}\"\n",
        "    while os.path.exists(output_path):\n",
        "        output_path = f\"{base}_{counter}{ext}\"\n",
        "        counter += 1\n",
        "    return output_path\n",
        "\n",
        "output_video_path = get_non_conflicting_path(output_video_base, output_video_ext)\n",
        "\n",
        "# Run the prediction\n",
        "results = best_model.predict(input_video_path, save=False, stream=True, conf=0.5)  # Do not save each frame individually\n",
        "\n",
        "# Open the input video to read frames\n",
        "input_video = cv2.VideoCapture(input_video_path)\n",
        "frame_width = int(input_video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "frame_height = int(input_video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "fps = int(input_video.get(cv2.CAP_PROP_FPS))\n",
        "\n",
        "# Define the codec and create VideoWriter object\n",
        "fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # codec\n",
        "out = cv2.VideoWriter(output_video_path, fourcc, fps, (frame_width, frame_height))\n",
        "\n",
        "# Define a dictionary to map class indices to colors\n",
        "class_colors = {}\n",
        "for cls in range(len(best_model.names)):\n",
        "    class_colors[cls] = tuple(int(c) for c in cv2.applyColorMap(\n",
        "        np.array([[cls * 255 // len(best_model.names)]], dtype=np.uint8),\n",
        "        cv2.COLORMAP_HSV)[0, 0, ::-1])\n",
        "\n",
        "# Iterate over each frame in the results and write to output video\n",
        "for result in results:\n",
        "    frame = result.orig_img  # Original image\n",
        "    boxes = result.boxes  # Get the detection boxes\n",
        "\n",
        "    if boxes is not None:\n",
        "        for box in boxes:  # Iterate over each box\n",
        "            x1, y1, x2, y2 = map(int, box.xyxy[0])  # Get the coordinates\n",
        "            conf = box.conf[0]  # Confidence\n",
        "            cls = int(box.cls[0])  # Class\n",
        "\n",
        "            label = f'{best_model.names[cls]} {conf:.2f}'\n",
        "            color = class_colors[cls]\n",
        "            cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n",
        "            cv2.putText(frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, color, 2)\n",
        "\n",
        "    out.write(frame)\n",
        "\n",
        "# Release everything when job is finished\n",
        "input_video.release()\n",
        "out.release()\n",
        "\n",
        "print(f\"Video processing completed. Saved to {output_video_path}.\")\n"
      ],
      "metadata": {
        "id": "efcrH6U2o-0L"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}